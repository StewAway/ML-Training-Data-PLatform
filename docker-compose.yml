services:
  # -------------------------
  # 1) Codegen (The Auto-compiler)
  # -------------------------
  codegen:
    build: .
    container_name: codegen
    volumes:
      - ./ingestion:/app
    working_dir: /app
    command: python3 -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. events.proto

  # -------------------------
  # 2) Kafka (KRaft Mode)
  # -------------------------
  kafka:
    build:
      context: .
      dockerfile: Dockerfile.kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_CLUSTER_ID: "dCHffFWYTCKWXiesmJMN9w"
    command: >
      sh -c "cd /kafka_2.12-3.6.2 && 
      if [ ! -f /tmp/kraft-combined-logs/meta.properties ]; then
        bin/kafka-storage.sh format -t $$KAFKA_CLUSTER_ID -c config/kraft/server.properties;
      fi;
      bin/kafka-server-start.sh config/kraft/server.properties 
      --override process.roles=broker,controller 
      --override node.id=1 
      --override controller.quorum.voters=1@kafka:9093 
      --override listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093 
      --override advertised.listeners=PLAINTEXT://kafka:9092 
      --override listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT 
      --override controller.listener.names=CONTROLLER"
    healthcheck:
      test: ["CMD-SHELL", "lsof -i :9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # -------------------------
  # 3) Producer
  # -------------------------
  producer:
    build:
      context: .
      dockerfile: Dockerfile.kafka
    container_name: producer
    depends_on:
      kafka:
        condition: service_healthy
      codegen:
        condition: service_completed_successfully
    volumes:
      - ./ingestion:/app
    working_dir: /app
    environment:
      TOPIC: "user_events"
    entrypoint: 
      - python3
      - event_generator.py
      - --bootstrap-server
      - kafka:9092
      - --topic
      - user_events
    command: 
      - --num-events
      - "10000"
      - --sleep-time
      - "0.1"

  # -------------------------
  # 4) Consumer (Python-based for Protobuf)
  # -------------------------
  consumer:
    build: .
    container_name: consumer
    depends_on:
      kafka:
        condition: service_healthy
      codegen:
        condition: service_completed_successfully
    volumes:
      - ./ingestion:/app
    working_dir: /app
    command: python3 consuming.py --bootstrap-server kafka:9092 --topic user_events

  # -------------------------
  # 4) Metrics (PENDING)
  # -------------------------
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

  # -------------------------
  # 5) S3-Compatible Data Lake (MinIO)
  # -------------------------
  minio:
    image: minio/minio
    container_name: datalake
    ports:
      - "9000:9000"    # API Port
      - "9001:9001"    # Web Console
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Create the bucket automatically on startup
  createbuckets:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 admin password123;
      /usr/bin/mc mb myminio/training-lake;
      exit 0;
      "
  # -------------------------
  # 6) spark master for streaming data into s3
  # -------------------------
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    volumes:
      - ./ingestion:/app
    ports:
      - "8080:8080"
      - "7077:7077"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip 0.0.0.0

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    depends_on:
      - spark-master
    volumes:
      - ./ingestion:/app
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  spark-streaming-job:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-streaming-job
    depends_on:
      - spark-master
      - kafka
      - minio
    volumes:
      - ./ingestion:/app
    command: >
      spark-submit 
      --master spark://spark-master:7077 
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 
      --py-files /app/events_pb2.py 
      /app/spark_streaming.py

  # -------------------------
  # 7) DDP training for topic by polling for data topic
  # -------------------------
  # --- DDP CLUSTER (4 Nodes) ---
  training-node-0:
    build:
      context: .
      dockerfile: Dockerfile.torch
    container_name: training-node-0
    environment: &ddp-env
      RANK: 0
      WORLD_SIZE: 4
      MASTER_ADDR: training-node-0
      MASTER_PORT: 12355
      GLOO_SOCKET_IFNAME: eth0
      GLOO_SOCKET_FAMILY: AF_INET
      OMP_NUM_THREADS: 2  
      MKL_NUM_THREADS: 2
      OPENBLAS_NUM_THREADS: 1
      VECLIB_MAXIMUM_THREADS: 1
      NUMEXPR_NUM_THREADS: 1
    volumes:
      - ./training:/app
    command: python3 train_wrapper.py --mode distributed

  training-node-1:
    build:
      context: .
      dockerfile: Dockerfile.torch
    container_name: training-node-1
    depends_on:
      - training-node-0
    environment:
      <<: *ddp-env  # Merge the common variables
      RANK: 1       # Override just the RANK
    volumes:
      - ./training:/app
    command: python3 train_wrapper.py --mode distributed

  training-node-2:
    build:
      context: .
      dockerfile: Dockerfile.torch
    container_name: training-node-2
    depends_on:
      - training-node-0
    environment:
      <<: *ddp-env
      RANK: 2
    volumes:
      - ./training:/app
    command: python3 train_wrapper.py --mode distributed

  training-node-3:
    build:
      context: .
      dockerfile: Dockerfile.torch
    container_name: training-node-3
    depends_on:
      - training-node-0
    environment:
      <<: *ddp-env
      RANK: 3
    volumes:
      - ./training:/app
    command: python3 train_wrapper.py --mode distributed

  # -------------------------
  # 8) Standard training (single node benchmark)
  # -------------------------
  benchmark-single:
    build:
      context: .
      dockerfile: Dockerfile.torch
    container_name: benchmark-single
    depends_on:
      - minio
    volumes:
      - ./training:/app
    command: python3 train_wrapper.py --mode single